---
title: "Intro to Random Forest"
author: "Adam Levin"
date: "8/8/2021"
output: html_document
---

```{r}
#A random seed allows for reproducable results
set.seed(17)

#Dataset size calculation
data_set_size = floor(nrow(iris)/2)

#random sqmple of data_set_size indexes
indexes = sample(1:nrow(iris), size = data_set_size)

#Assign to training and validation sets
training = iris[indexes, ]
validation1 = iris[-indexes, ]

#_____
#import randomForest
library(randomForest)

#Perform training:
rf_classifier = randomForest(Species ~ ., data = training, ntree = 100, mtry = 2, importance = TRUE)

#visualization of importance
varImpPlot(rf_classifier)

#training against validation1 set
prediction_for_table = predict(rf_classifier, data = validation1[, -5])
#confusion matrix to check how good classifier is
table(observed = validation1[ , 5], predicted = prediction_for_table)

#Validation set assesment #2: ROC curves and AUC

#Need to import ROCR package for ROC curve plotting:
library(ROCR)

#Calculate the probability of new observations belonging to each class. prediction will be a matrix  w/ dimensions of data_set_size and number_of_classes
prediction_for_ROC_curve = predict(rf_classifier, validation1[ , -5], type = "prob")

#colors
pretty_colors = c("#F8766D", "#00BA38", "#619CFF")
#specifiy classes
classes = levels(validation1$Species)
#For each class
for (i in 1:3)
{
  #define contributions for class[i]
  true_values = ifelse(validation1[ ,5]==classes[i], 1, 0)
  #assess performance of classifiers for class[i]  
  pred = prediction(prediction_for_ROC_curve[, i], true_values)
  perf = performance(pred, "tpr", "fpr")
  if (i==1)
  {
    plot(perf, main = "ROC Curve", col = pretty_colors[i])
  }
  else 
  {
    plot(perf, main = "ROC Curve", col = pretty_colors[i], add = TRUE)
  }
  #calculate AUC and print to screen  
  auc.perf = performance(pred, measure = "auc")
  print(auc.perf@y.values)
}

```